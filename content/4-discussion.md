# Discussion {#sec:discussion}

## Introduction

In this chapter, we aim to take a broader view on what would be necessary to achieve a fuller understanding of the processes at work in cultural change at the linguistic level.
So far we have adopted wholesale the paradigm put forward by Cultural Attraction Theory, by seeking to identify and elucidate situations where linguistic representations are transformed as they are transmitted, and assessing, on one side, the extent to which the empirical evolution of content agrees with what is expected under CAT, and on the other side, the extent to which CAT provides productive guiding questions in understanding what is at work in the situations studied.
This has led us to identify a number of behaviours which are consistent with Cultural Attraction Theory:
studying word substitutions in online quotations first, and more general transformations in controlled transmission chains of short utterances second, we showed that the lexical features of words evolve in a systematic manner to make utterances easier to produce, and that the direction of the evolution is consistent with the attraction pattern that can be observed in the individual step of word replacements.
However, these approaches did not bring us any closer to understanding the semantic changes that utterances undergo when they are transformed, be it online or in controlled transmission chains.

We now wish to discuss the reasons for this limitation.
Our purpose is first to convince the reader that meaning
^[In what follows, we will always assume that meaning is meaning *to someone*.
For instance, in a communication situation, meaning to a listener is the listener's interpretation, and meaning to a speaker is the meaning intended to be communicated.
]
is a crucial aspect in the evolution of content which must eventually be analysed in order to fully understand the way representations circulate and change.
Manual exploration of the changes in transmission chains in particular show that the surface measures that we used in quantitative analysis have no handle on the evolution of meaning.
Indeed, meaning will appear as a deeply context- and interaction-dependent property, which cannot be understood by simply focusing on the utterances themselves.
We will thus discuss two important approaches to studying the meaning of utterances in relation to the context and interaction in which they appear:
Relevance Theory and the Enactive approach.
The first is more mature and better integrated with other areas of linguistics.
Its philosophical basis is the same as CAT:
it follows the computational metaphor of mind, basing its account of meaning on the symbolic processing of mental representations.
The second is younger, and is philosophically based on authors who have strongly criticised representational approaches to cognition.
It starts from a dynamical and more continuous notion of meaning which avoids some of the issues with mental representations, but has yet to tell a complete story for the study of language.
These two approaches illustrate opposite ends of the spectrum of approaches to meaning.
Favouring one or the other, or possibly combining parts of the two, will undoubtedly be related to one's overall construal of cultural evolution and to the importance of representations in a theory of cultural change;
we will not venture into that territory however, and will limit ourselves to exposing alternatives available to empirical approaches.
Indeed, we believe that informed empirical investigation has an important part to play in the question of meaning in cultural change and the debates it relates to.
After having laid out these two alternatives, then, we propose some speculation on practical approaches that could contribute to the empirical side of this question.

We begin by discussing detailed examples of the role of semantics in our transmission chain experiment, to show how the lack of an account of utterance meaning renders the empirical question of attractors in this case under-specified.
Next, we present in more detail the two ends of the spectrum of approaches to meaning, and discuss their relationship to an overall view of cultural change.
Finally, we present possibilities for advancing the debate through empirical investigation.


## Empirical epidemiology of linguistic representations {#sec:discussion-empirical}


### Relevant results {#sec:discussion-empirical-results}

The path we took so far has consisted in entirely adopting the cultural attraction paradigm and developing experiments to evaluate one of its strong hypotheses, namely the existence of attractors in the evolution of representations.
Indeed, cultural attractors are in many ways a cornerstone for the theory, as they reflect its explanation of the stability of culture in spite of strong micro-level transformations (they are the product of ecological and psychological factors interacting with each other), and they provide intelligibility into the complexity of cultural change as a whole.
Linguistic utterances appeared as a good proxy to study representations that are part of everyday life and for which large corpora are readily available.
Language is also one of the most versatile means by which representations circulate, making linguistic utterances an important study case for the theory.

Our initial high-level question was thus whether attraction could be observed in the evolution of linguistic utterances as they are interpreted and produced anew by successive people.
The first case-study we developed relied on online quotations, a type of representation for which an implicit rule mandates perfect copy, yet which often changes as it propagates across blogs and news outlets.
Our investigation of single-word replacements showed that, when transformed, words are reliably replaced by words easier to produce.
Evaluated on standard lexical features, individual word replacements showed an attraction pattern specific to each feature and consistent with the hypothesis of an attractor at the lexical level, which could be due to cognitive biases in word production.
Our second case-study explored utterance transformations in a more controlled situation, by setting up artificial transmission chains of short utterances on an online platform.
Here, the analysis first focused on developing a descriptive model that would provide an overview of transformations decomposed into more basic operations.
We showed that transformations can be reliably described as made of chunks of word insertions and deletions interspersed with individual word replacements (as well as chunk exchanges, which were set aside for the analysis).
This level of description showed that the transformation process has several regularities:
operations strongly depend on each other (in particular, insertions appear to make up in size for some of the deletions, while still introducing substantial change), and their prevalence also depends on the length of, and their position in, the utterance (longer utterances receive more operations; replacements preferentially target the interior of utterances, and insertions and deletions the second half of utterances).
The behaviour of insertion and deletion chunks, as well as replacements, was shown to be consistent with the biases identified in individual replacements in online quotations:
the susceptibilities for being targeted by deletion or replacement, and appearing by insertion or replacement, closely complemented each other, in accordance with the hypothesis of an attractor at the lexical level;
the overall evolution of the lexical makeup of utterances also reflected those biases by drifting in a specific direction on each lexical feature (corresponding to easier recall).
More generally, we argued that the modeling approach provides crucial detail about the transformations, achieving a middle-ground between the focus on lexical features in individual word replacements and the wide-angle view of contrasts in the aggregated evolution of content along chains.

These analyses were made at the cost of several trade-offs.
Transformations in the online data set were restricted to single-word replacements so that we could infer missing source-destination links between quotations, and lack of data meant that no analysis could be made of the context surrounding the quotations.
The transmission chain experiments were led with an extremely simple read-and-rewrite task (though this was an intentional first step), which also did not open the analysis to the role of context in transformations and in the overall evolution of content.
Nonetheless, these studies demonstrate that it is possible to decompose the transformations of utterances into combinations of smaller operations, and fully connect the behaviour of those operations with known effects in psycholinguistics, be it online (with a partial view of the process) or in controlled transmission chains (with a full view of the transformations).
They further suggest that, due to cognitive biases in the way utterances and words are recalled, the evolution of short utterances like quotations could be subject to an attractor at the lexical level, making the words of utterances gradually easier to recall, on top of other changes in the actual content conveyed.


### Challenges {#sec:discussion-empirical-challenges}

However, these studies do not tell us the way utterances evolve semantically.
Indeed, apart from the vector-based comparison of individual words for scoring matched and mismatched pairs in utterance alignments (an arguably simplistic approach to word comparison), none of the analyses we put forward have a grip on the meaning of the utterances, and much less on the change in meaning upon transformation.
While it is noteworthy that it was still possible to extract reliable decompositions of the transformations without such information (as the manual evaluation of alignments attests), these analyses are blind to changes in the content circulated by the utterances.
^[We also explored the semantic distance traveled by words upon replacement, and the possible hyponym-hypernym relationships between replaced and replacing words, but did not present the analyses in the previous chapter as they provided no additional insight about the process.
]

Let us show a few examples of the types of meaning change that were observed in the transmission chain experiments of the previous chapter.


#### Minor operations can change the function of a part of the utterance

Consider the following root utterance from in Experiment 2:

\begin{nquote}\label{u:party} % <!-- #49@2 -->
  "Can you think of anything else, Barbara, they might have told me about that party?"
\end{nquote}

The second part of this sentence is slightly misleading, and could be seen as a mild case of garden path sentence:
^[A garden path sentence is a sentence that misleads the reader into parsing its syntax one way, but necessitates a different structure to be understood once all the words have been read.
A classic example is the sentence "The horse raced past the barn fell", which misleads the reader into interpreting "The horse" as the subject of "raced";
the correct parse corresponds to the meaning of "The horse that was raced past the barn fell", where "The horse" is the object of "raced".
The difficulty comes from the fact that the search for the correct parse becomes necessary only once the reader has seen the final word, "fell".
]
to "tell about" can be either transitive or intransitive, and while the final "that party" determines it as a transitive verb (for which it is the object), several subjects in the experiment rewrote the following sentence:

\begin{nquote} % <!-- 49#257@2 -->
  "Can you think of anything else, Barbara, they might have told me about \textbf{at} that party?"
\end{nquote}

The added "at" turns "tell about" into an intransitive verb, and turns the final part of the sentence into an adverbial phrase of time, thus changing its function in the sentence.
More importantly, the sentence in its new form implies that the speaker was at the party, whereas the original sentence implies the contrary (although one could imagine the speaker was present but does not remember the details of the party).
There is therefore a substantial change in the high-level meaning of this utterance, through the addition of a single word which changes the function of part of the utterance.

Once this change has occurred, regularisations often happen in the rest of the branch, for instance removing "about" to turn the ending of the sentence into "told me at the/that party".
Looking at the leaves of the seven branches this tree contains, 4 of them maintain the implication that neither the speaker nor Barbara were at the party, 2 imply that the speaker (and not Barbara) was at the party, and one implies that Barbara was at the party (and the speaker was not).


#### Minor operations can create an ambiguity triggering larger changes

As we discussed at the end of the previous chapter, minor changes can also lead to larger downstream consequences in surface representation (as well as in meaning).
An example of typographical error similar to the previous chapter can be seen with the following sub-chain in Experiment 3 (putting aside the UK/US spelling change, "canceled" $\rightarrow$ "cancelled"):

\begin{nquote} % 8#1376
  "The charge of embezzlement against the artillery has been canceled."
\end{nquote}
\begin{nquote} % 8#1696
  "The charge of embezzlement \textbf{again} the artillery has been cancelled."
\end{nquote}
\begin{nquote} % 8#1793
  "The charge of embezzlement again, the charge has gone."
\end{nquote}

In this case, the "against" $\rightarrow$ "again" replacement operated in the first transformation leads the following subject to interpret the sentence quite differently, making a larger change.
This behaviour is far from systematic, as many times such small errors are corrected by later subjects.
Consider for instance the following error made by a subject in Experiment 2:

\begin{nquote} % 11#248@2
  "At least when they say they're going to have a war, they keep their word."
\end{nquote}
\begin{nquote} % 11#666@2
  "At least when they say they are going to have a war, they keep \textbf{there} word."
\end{nquote}

The "their" $\rightarrow$ "there" replacement is maintained by the next subject, but then reverted by the one after that, thus coming back to the original sentence (aside from the change in contraction, "they're" $\rightarrow$ "they are").


#### Weak and strong pragmatics

The examples above, and the variability they illustrate, testify to the fact that different subjects can interpret the same utterance in strongly divergent ways.
Subjects do not only differ on their performance in accurately reproducing the utterances presented, their productions also signal that different meanings can be constructed from the same root utterance (such differences accumulate, as was illustrated by the divergence of branches observed in the previous chapter).
Part of this observation is commonplace, as the meaning of an utterance depends in obvious ways on the context in which it is produced or read, such as when deictics are used (words such as "today" or "here", which are context-bound by nature).
However, most isolated utterances are under-specified in a way that makes them much more dependent on the context and on the interaction they appear in then what deictics suggest.

Consider once again utterance \ref{u:party}.
With no further context, it is not clear what party the speaker is referring to, who were the participants, or why the speaker is asking about it.
As the interpretations made by subjects in Experiment 2 illustrate, one could imagine that the speaker was at the party but does not recall its events, or that Barbara witnessed someone telling the speaker about the party, a telling that the speaker would not recall, and so on and so forth with other hypotheses.
The sentence is originally extracted from a movie script, and in this case the sentence immediately preceding it in the script is enough to drastically reduce the possible interpretations:

\begin{nquote} % http://www.imsdb.com/scripts/Devil's-Advocate.html page 9
  "I've spoken to the other children who were there that day. Can you think of anything else, Barbara, they might have told me about that party?"
\end{nquote}

With this minimal context, it is now clear (or at least much more likely) that the speaker was not at the party, but is asking Barbara to tell him or her something he or she already knows.
To fully understand the utterance however, much more information about the interaction is needed:
one must know that the utterance, extracted from the 1997 movie "The Devil's Advocate", is pronounced by a lawyer defending his client, a sexual abuser, while accusingly questionning Barbara, a victim of the abuser and witness in his trial.

This example illustrates what @scott-phillips_pragmatics_2017 calls *strong* pragmatics.
Contrary to *weak* pragmatics, which construes the context-dependence of meaning as a layer to be added on top of semantics, syntax, morphology, phonology and phonetics, strong pragmatics refers to the fact that all communication fundamentally depends on social cognition, which cuts through the other layers of linguistic analysis such as semantics and syntax.
Indeed, what is communicated through the utterances discussed above, which can be rephrased as "tell me this thing I already know but that the audience does not", could have been conveyed through an entirely different set of sentences (that is different semantics, syntax, morphology, and so on) because it depends above all on the social cognition situation that participants find themselves in.

Examples of this phenomenon abund, and are not restricted to face-to-face interactions as depicted by films:
no matter the type of mediation, any interaction is likely to feature strong pragmatics.
Twitter conversations are a good case in point for online platforms.
The short conversation reproduced below, for instance, illustrates the fact that the meaning as understood by participants is a construction depending on context, past history, and interaction dynamics.
^[The conversation is originally in French, and reads as follows:
"On est tous le beau et le moche de quelqu'un" / "mais être moche c'est quand même la base ahah" / "[mort de rire,] pour certaines filles surtout, je pense".
]
It starts with the following tweet:

\begin{nquote}
  "We are all good-looking and ugly to someone else's eyes"
\end{nquote}

This utterance seems a priori neutral, and is commonplace and consensual enough for it to be marked as favorite, retweeted and published anew regularly.
^[A simple search on Twitter using the original text in French shows that the utterance appears about once a month, with most instances retweeted several times.
]
But as illustrated by the answers following it, the actual meaning exchanged in the conversation is not available to the non-interacting reader.
A first answer is made in a humourous tone:

\begin{nquote}
  "but we're still ugly in the first place haha"
\end{nquote}

Then, two replies later, the conversation ends:

\begin{nquote}
  "[laughing out loud,] true for some girls especially, I would say"
\end{nquote}

Even after five replies, we cannot determine whether the meaning exchanged is about sexism and rejection, or simply a flimsy joke without consequence;
yet when taken as cultural tokens, these two representations are diametrically opposed to each other.
With no further information about the relationship between the participants, their past interactions or common history, and in spite of the conversation being entirely public, we cannot determine what the exchange is fundamentally about, or even decide what the initial tweet means to one participant or the other.


#### Summary of problems

Let us now return to our initial question, namely the identification of attractors in the evolution of linguistic content.
As might be clear by now, this goal is challenging in at least three new and related ways.
First, the importance of strong pragmatics renders it much more difficult to collect all the necessary data to understand the meaning that a subject attributes to an utterance, or what is exchanged in an interaction.
Indeed, it is often necessary to rely on detailed information about the interactive situation to understand that meaning.
Leaving aside the question of the theoretical and technical apparatus that would be required to quantitatively analyse such data, the situations in which an experimenter can have access to the whole interactive situation, and thus have access to meanings exchanged (i.e. determine the content of the representations that circulate), are extremely rare.
In most cases, an experiment only gives access to artefacts that are part of a broader cycle of meaning creation.

Second, even when the interactive situation is available to observation, the meaning of an utterance is not reducible to a simple object, and remains a multi-scale (and inside each scale, multi-dimensional) target.
Coming back once again to utterance \ref{u:party} with its surrounding context, what aspect of the meaning should one focus on when examining its evolution to identify attractors?
The request to publicise private information, the implication that Barbara is lying or holding back such information, the semantic or syntactic-level formulation of the question?
In other words, the goal of identifying attractors in the evolution of meaningful utterances is, at least in our current formulation, under-specified.
This problem is not new, and might even not be a theoretical problem for Cultural Attraction Theory:
behind the multi-dimensionality of meaning is the fact that culture itself is a multi-scale phenomenon (and multi-dimensional at each scale), difficult to characterise in simple mathematical terms.
CAT works around this problem, as Sperber insists that it should not aim for a "grand unitary theory", and should rather generate useful domain-specific questions that depend on the matter at hand [@sperber_explaining_1996 pp. 61, 83].
@sperber_explaining_1996 [pp. 53-54] also notes that "our access to the content of representations is unavoidably interpretive", and that CAT does not solve that problem, but instead provides tools to make our coping with the problem more reliable.
Thus, for CAT, the empirical decision of which meaning level to focus on depends on what one is looking for, and must be resolved by looking at the importance of each level as individually observed.

Finally, a more important theoretical challenge comes from the fact that strong pragmatics puts an important part of the responsibility for meaning, that is for the content of a linguistic representation, in the interactive situation itself.
If the meaning of an utterance is determined in great part by the interaction it features in, and if that meaning corresponds to the content of the linguistic representations whose epidemiology we wish to study, then how is it possible to identify two representations from different situations as being the same (or being close to each other)?
To make progress in the epidemiology of meaning-bearing utterances, an approach to strong pragmatics must thus be able to relate meanings that come from different interactive situations, to some extent at least.
Indeed, evaluating the evolution of representations requires us to be able to identify, if not the path taken by specific strands of representations which inherit from each other, maybe the positions of a population of representations in a common state space, or at the very least the extent to which two representations differ from each other, in order to reconstitute such a state space.
As a consequence, an approach to pragmatics useful to CAT must provide a way to declare meanings different, or identical, or evaluate the extent to which they differ, across situations (without which evolution can only be observed inside fixed interactive situations).


## Approaches to meaning

To make progress, therefore, we must broaden the scope of empirical studies of CAT beyond interactionless transmission chains and consider all interactions, either face-to-face or digitally mediated, and not necessarily linguistic.
We are then faced with the concrete question of how to understand the way an agent (human or non-human organism) extracts or constructs meaning in such an interaction.
That is, which of the infinite possible meanings the agent selects (or constructs), and how that selection (or construction) operates.
As we just saw, such meaning is highly dependent on the context and interaction the agent finds itself in, such that viable approaches to meaning will necessarily be coping with the complexity of possible interactive situations.
This makes the picture considerably more complicated than when dealing with simple context-free utterances.

In this section, we present two prominent approaches to meaning and pragmatics, both of which can prove useful for further exploration of cultural evolution.
The first, Relevance Theory, fleshes out the idea [first introduced by @grice_studies_1989] that human communication is ostensive communication, based on the recognition of relevant communicative intentions.
The second approach, enactive, starts from a more bare-bones level of description and proposes an understanding of how meaning emerges from the interaction of agents seen as dynamically coupled organisms.
As we will see, both these theories provide (part of) an answer to how agents select, infer or construct subtly varied meanings in the course of an interaction, but they do so by starting from opposite ends.
The first builds on a propositional notion of representations that are combined in inference processes, while the second starts from a representationless description of organisms whose interaction and coupling with the environment endogenously generate (non-representational) meaning.
The notions of meaning to which they arrive are quite disjoint:
the first is mechanistic-representational, and the second is dynamical, two explanatory approaches that have historically been considered in contradiction.
However, more recent work suggests that mechanistic and dynamical explanations can be usefully combined to describe one same system [@chemero_after_2008].
Whichever the case, we will argue that in their current state both accounts of meaning can be productive guides for generating empirical questions and experiments regarding cultural evolution.


### Relevance theory


#### Principles

As a general theory of communication, Relevance Theory has a very broad scope and relates to many areas of cognitive science.
@sperber_relevance:_1995 and @wilson_truthfulness_2002 provide detailed presentations of the full theory, and many publications in between and since then have fleshed out its relations to a number of neighbouring questions.
The collection of works by @wilson_meaning_2012, in particular, provides a thorough discussion of several linguistic phenomena based on Relevance Theory, as well as openings towards experimental and cultural evolution-related approaches to the question of meaning and relevance [for language evolution see in particular @sperber_pragmatic_2012].
Here we will restrict ourselves to a sorely abridged presentation of the already summarising @wilson_relevance_2004, in the hopes that it will be enough for an understanding of the principles underlying the theory and the explanations it provides.
^[Our presentation focuses on the founding principles of Relevance Theory.
The remainder of @wilson_relevance_2004 fleshes out the way the inferential procedure is applied to linguistic utterances, how the theory explains typical phenomena such as irony, poetry, or loose uses of language (e.g. the meaning of 'square' in expressions such as 'square face' or 'square mind'), and how it fits with the massive modularity of mind approach introduced in @sperber_explaining_1996, along with many detailed examples.
]

Relevance Theory (RT) opposes itself to the code model of linguistic communication, according to which a speaker's meaning is encoded in an utterance, passed on to the listener by means of a channel or conduit (for instance sound), and then decoded by the listener to obtain the communicated meaning.
By contrast, RT adopts an inferential model:
as the semantics of utterances provide only under-determined information about the speaker's meaning (as illustrated by the examples from the previous chapter discussed above), an utterance is not considered to encode a meaning per se;
instead, the inferential model considers that utterances provide evidence (and exactly the right amount of evidence) for the intended meaning to be inferred given the situational context.
This model of communication was first elaborated by Grice, building on the fact that people who are communicating usually assume that what the other person is saying is meant to be understood given the context at hand;
in other words, people take their interlocutors to be neither stupid nor adversarial, and assume (consciously or not) that what a speaker says is a signal for a meaning that the listener should be able to understand, through inference.
Grice thus identified four general rules (maxims) that listeners generally assume their interlocutor will follow, and on which they rely to infer meaning:
Quality (truthfulness), Quantity (informativeness), Relation (relevance), and Manner (clarity).
RT agrees with the intuition behind Grice's observations (although it differs on exactly which listener expectations should be necessary), and fleshes out this general inferential model of communication in cognitively plausible terms.

RT proposes that inferential communication is based on a cost-reward comparison of possible conclusions that derive from a speaker's utterance.
Indeed, a given utterance (or non-linguistic communicative act) in a given context can lead a listener to any number of conclusions about the world.
Each of those conclusions about the world can matter more or less to the listener (RT formulates this as the strength of the contextual effects created by the conclusion), and is also more or less costly to derive from the speaker's utterance and its context (processing cost in RT terminology).
A conclusion that matters more to the listener achieves higher relevance, and conversely a higher processing cost will lower the relevance realised by a conclusion.
These two dimensions let listeners order the conclusions that can be derived from a speaker's utterance based on their (context- and listener-dependent) relevance.
For instance, Sperber or Wilson hearing that their train to work is one minute late provides a less relevant conclusion (because it matters less to them) than hearing that their train is late by half an hour.
Conversely, a public announcement stating that their train is late provides a more relevant conclusion (because easier to derive) than the same conclusion derived with more deductive effort from parts of a conversation overheard between the people sitting next to them.
A central claim of RT is that evolution has shaped human cognition in such a way that people automatically and easily perform this derivation and comparison process on all the stimuli they perceive, picking out those among the myriad available which maximise relevance.
The Cognitive Principle of Relevance expresses this claim:
"Human cognition tends to be geared to the maximization of relevance" [@wilson_relevance_2004 p. 610].

@wilson_relevance_2004 then define inferential communication as consisting of two elements.
An *informative intention*, that is the intention of a speaker to inform an audience of something (more precisely, to make certain assumptions more, or less, manifest to the listener), and a *communicative intention*, that is the speaker's intention to inform the audience of their informative intention.
In other words, inferential communication happens whenever the speaker says (or does) something in order to make her audience recognise that she wants to convey X.
The meaning is successfully understood when the audience recognises the speaker's informative intention, that is when the audience recognises that the speaker wants to convey X (note that X itself might not be conveyed if the audience does not trust the speaker -- the communication event is nonetheless successful, since the intention to convey X was recognised).
Most often, the speaker does this by making an ostensive communication act (e.g. pointing, staring, or saying something that attracts the audience's attention) which signals to the audience that there is something worth processing to attend to.
Indeed, ostensive stimuli create in the audience an automatic expectation for relevance, as the audience looks for the reason for which the speaker is attracting their attention.
More precisely, RT posits that the audience automatically expects the stimulus to be *optimally* relevant;
in the theory's terminology, this is formulated as the Communicative Principle of Relevance:
"Every ostensive stimulus conveys a presumption of its own optimal relevance."
This principle is the basis on which the audience's inferential process works:
the speaker's ostensive stimulus signals something worth processing to reach a relevant conclusion (since she attracted their attention to process it), and it is also the stimulus that makes that relevant conclusion the easiest one to reach.

The authors illustrate this with an example:
we are at a table and my glass is empty, a fact that you might notice.
If you do (without me communicating anything), one conclusion you could reach is that I might like a drink.
If, however, I wave my glass at you, or say "My glass is empty" (ostensive stimulus attracting your attention to my empty glass), a relevant conclusion you would reach is that I want to communicate that I want a drink.
If you trust me, you might conclude that I want a drink, although the communication will have succeeded even if you do not.
RT thus proposes a procedure that can account for the way utterances are understood:
when perceiving a stimulus in given a certain context, compute the relevance of its conclusions following a path of least effort first (since the stimulus is expected to be optimal), and stop whenever you have reached your expected level of relevance.
In other words, test hypotheses about the speaker's utterance such as possible disambiguations, resolution of entities and implicatures, and stop whenever the conclusions you have reached seem relevant enough to you.
The conclusion you have then obtained will be your assumption of the speaker's meaning for the context chosen at the beginning.
Finally, note that this inference procedure can be operated in different possible contexts, as long as they are available to the listener given their memory constraints.
The remainder of the procedure thus optimises on the context in which conclusions are drawn, selecting the context for which the final conclusion is most relevant.


#### Application

The framework provided by Relevance Theory is extremely rich, and has been the subject of extensive experimental exploration and validation [see @noveck_why_2012; and @van_der_henst_testing_2012 for reviews].
In particular, it brings direct insight to some of the limitations concerning meaning in the experimental approach of the previous chapter.
The transmission chains we set up are a clear case of ostensive communication:
we ostensively ask the subjects to direct their attention to utterances to memorise and rewrite.
However, the utterances are presented with no context other than the task itself, which frames the experiment as a memory exercise.
There is no background information against which the subjects can evaluate the relevance of conclusions derived from the utterances, nor are the subjects involved in an activity that would make the conclusions matter in one way or another.
Without an ecological activity to which the utterances can become relevant, the experiment has no control over the meanings that subjects will infer from the utterances, leaving the matter entirely under-specified.
It is also easier to understand why subjects spontaneously wrote sentences directed to the experimenter such as "I can't remember":
if the task does not create an ecological communicative activity that relates subjects to each other, and is instead (correctly) understood to be a memory task, the only valid interlocutor is the experimenter evaluating the memory performance.
As a consequence, asking the subjects to keep in mind that their productions were sent to other subjects (as we did) was likely to create a slight discrepancy.

The relevance-theoretic approach also opens the door for the analysis of interaction, context, or past history in the evolution of meanings.
Using carefully constructed contexts to orient what is available to the inferential process, it may be possible to greatly reduce the possible meanings interpreted by subjects, and thus explore the way interpretations evolve through chains of contextually-augmented utterances (or chains of constrained interactions).
In practive however, such an implementation is likely to be extremely challenging (much more so than the procedure developed in the previous chapter):
whatever the theory, it is still necessary to extract the basic propositions semantically encoded in the utterances typed in by subjects, determine the way subjects select contexts, and automatically or manually derive the possible conclusions that can be reached for a given utterance in a given context.
The three tasks are far from trivial.
Second, efficiently constraining the context in which an utterance is interpreted will likely be much more difficult than it sounds, as real life interpretation involves our own personal history, memory, preoccupations and any other pregnant contexts we can recruit during the process [see @sperber_relevance:_1995, §3.3-4, which discusses the ways contexts are chosen in the inferential process].
It is doubtful that simply adding a few sentences around the target utterance would suffice.
Using more encompassing situations, such as a controlled video game where much larger parts of the many contexts available to subjects can be experimentally manipulated, might help in this area.


#### Meaning as indexed on knowledge optimisation

We have just seen that the notion of meaning provided by Relevance Theory is based on a maximisation of the relevance of conclusions derived by the listener.
This account ultimately rests on the three following cognitive mechanisms:

- Reconstruction of the logical form of an utterance, in order to start the inferential process [see @sperber_relevance:_1995, §4.3, for details],
- Creation or selection of contexts inside which the inferential process operates [see @sperber_relevance:_1995, §3.3-4, for details],
- The inferential process itself, operated by what is hypothesised to be a special-purpose deductive device [see @sperber_relevance:_1995, §2.4-5, for details].

This account can also be formulated in terms of a system which optimises its representation of the world without access to the truthfulness of what it perceives;
in this case relevance indicates the path of strongest optimisation growth.
To see this we must briefly return to the exact definition of relevance in the theory.
Relevance in @sperber_relevance:_1995 is defined in the following three summarised steps:

1. Define the *deductive device* that is used for inference:
   it is a mechanism for deriving conclusions from a set of premises P (usually coming from an utterance from a speaker) in a set of contextual assumptions C [@sperber_relevance:_1995, §2.4-5];
   in this device, all assumptions, premises and conclusions have a certain strength, corresponding to their level of accessibility (this is not a logical measure of confidence that can be quantified, but rather an ordinal property that can be used to compare assumptions between each other).
2. Define the notion of *contextual effect*:
   a set of premises P (coming from an utterance) in a set of contextual assumptions C generates a contextual effect if and only if the deductive device can derive conclusions from the combination of P and C that it cannot derive from P or C alone.
   Such conclusions can be new to C, can strengthen existing assumptions in C, or can weaken and even erase assumptions in C [@sperber_relevance:_1995, §2.6-7].
   The notion of contextual effect thus provides an indication of the strength of the relationship between an utterance and a set of contextual assumptions C.
3. Define the degree of *relevance* realised by conclusions derived by the deductive device from P and C:
   conclusions are more relevant if they have stronger contextual effects, and less relevant if they have higher processing costs (in terms of deductive steps involved).

With relevance thus defined, Relevance Theory's procedure for interpreting utterances and other stimuli can be seen as a tentative procedure to always improve the system's representation of the world:
if the processing system has no access to the truthfulness of the stimuli it perceives, the best way to use incoming information is to combine it with the assumptions that will benefit from it the most, for a given level of trust in the speaker.
If the system functions with a deductive device such as the one defined above, this corresponds to finding the set of contextual assumptions C on which the new information (premises P) has the strongest contextual effect, with given processing cost constraints and for a given level of trust in the speaker (i.e., strength of the premises P);
given the above definition of relevance, that is precisely the procedure to reach the most relevant conclusions.
Under this description, higher relevance indicates a stronger update in contextual assumptions, thus a stronger update to the system's overall representation of the world.
Thus Relevance Theory can be seen as indexing meaning on the increase in reliability of a listener's representation of the world.
The Communicative Principle of Relevance then simply states that speakers know that listeners work by maximising inferred relevance, and will behave accordingly in order to be understood, that is by saying things which they know will trigger the right inferences for the listeners.

Whichever the formulation we choose, RT crucially relies on a deductive device which can represent propositions extracted from utterances and process them in order to derive new conclusions.
Once the inference process is completed, the meaning defined by RT is partially propositional, as it is made of a set of new assumptions that are made more less manifest (through a change of strength, which in itself is not propositional).
Such meanings can thus be at least partially defined and used without reference to the situation they were deduced from, making them partially comparable to other meanings from other situations.
The approach to pragmatics developed by RT is thus quite amenable to the study of cultural evolution (which is no surprise, given the authors).

One disadvantage of the relevance-theoretic approach is the high level at which it starts its description, making concrete implementations more challenging.
As we indicated earlier, it relies on the reconstruction of the logical form of utterances, on a mechanism for the creation of contexts, and on the inferential process itself.
For non-linguistic interactions in particular, it is not clear how logical forms can be extracted from the stimuli.
Second, a theoretically important, but experimentally less important consequence of the theory's reliance on propositions and mental representations is that a full account of meaning will eventually require an explanation of what exactly representations represent, and how they do so.
This question has raised much debate in philosophy of mind, where critical authors in particular call it the "Hard Problem of Content" [@hutto_radicalizing_2013].

We now turn to a second approach to meaning, based on a dynamical account of interactions which does not rely on propositions or representations.
The account of language that this approach provides is still being developed, and is thus less connected to other areas of linguistics.
Nonetheless, in its current state it already provides powerful conceptual tools which can be contrasted with RT to explore the question of meaning in cultural evolution.


### The Enactive approach

#### Accounting for the feel of linguistic meaning

The Enactive approach proposes a different foundational metaphor for the study of cognition.
This contrasts with Relevance Theory and Cultural Attraction Theory, as well as our own experimental approach to its questions, who rely on a computational metaphor for the description of cognition:
the mind is seen as a computer, that is an information-processing device which continuously receives stimuli, updates an internal representation of the world based on what it perceives, and acts based on its current representation and predictions.
For the computational metaphor, the human brain is our implementation of such an information-processing device.
One problem with this approach is that it has difficulties accounting for the way cognition *feels*.
The metaphor is productive to understand the detailed workings of a cognitive mechanism, but it gives little insight into how a cognitive system can feel things, and why exactly things feel the way they do (or for instance, why some things feel different from other things, and how exactly to describe such differences).
To put it bluntly, computers do not feel, and if they did it is not clear that their feelings would reduce to properties of the representations they hold about the world.
In theory, this shortcoming of the metaphor might be avoided by working around the problem or arguing that the question is ill-defined.
However it is a crucial point for linguistic utterances:
understanding the meaning of an utterance has a non-trivial phenomenological aspect, which is likely to impact the way utterances are transformed.
Understanding the meaning of an utterance can be so intimate and so particular that we could see it as an instance of *feeling* a meaning in a particular situation.
In that case, remembering an utterance's meaning is bound to be affected by the way it was initially felt.
A better account of the way utterances change when they are produced anew will thus likely require an understanding of the way utterances are felt, as well as how and why different utterances produce different feelings.

The Enactive approach to cognition proposes an explanatory path to account for the way linguistic meaning feels, and for the way such feelings can be gradually structured into forms similar to propositions.
It is part of a broader range of approaches that question the utility of seeing the mind as an information-processing system,
^[These approaches are sometimes collectively termed the "E turn", in reference to the many titles starting with the letter "e" (in particular, enactive, embedded, embodied and extended approaches to cognition).
]
and explore the extent to which parts of (or all) the metaphor can be relaxed or replaced by other paradigms [see @chemero_after_2008 for a review of the options available in the debate, and the fields corresponding to each choice].
In this area, the Enactive approach has the advantage of being extremely consistent in its rejection of the computational paradigm and of the idea that cognitive systems represent their environment, and thus proposes a radically different account of pragmatics than the one developed by Relevance Theory.
Indeed, instead of relying on a notion of representation, it proposes to account for cognition in terms of the dynamical coupling of organisms with their environment and with each other.
Compared to RT, it uses a simpler initial level of description and locates itself at the opposite end of the computational spectrum;
still, both aim to reach complete and plausible explanations of language and meaning.

As @chemero_after_2008 argue, these two types of approaches are not necessarily opposed, and could be usefully combined to form complementary explanations.
Our goal here is to present the basic tenets of the Enactive approach and to show how, by starting with a different metaphor, it faces an orthogonal set of problems compared to RT.
In particular, the notion of meaning it develops is more endogenous than that of RT (among other things, by being non-representational, it does not face the Hard Problem of Content), but is not yet usable to fully comprehend linguistic interactions.
What the theory currently provides can be seen as the explanation of preliminary steps common to language and less structured interactions, eventually to grow into a full theory of linguistic interactions (or, in enactive terms, "enlanguaged" interactions).

The first concrete articulation of this approach in cognitive science is usually attributed to @varela_embodied_1991 who develop a view of cognition based on Merleau-Ponty's phenomenology.
They propose to look at mind, cognition and meaning as fundamentally embodied and situated processes in which self-organisation plays a central role, and aim to account not only for the way cognition works, but for the way cognition feels to cognisers.
Of course, nobody basing themselves on the computational paradigm would deny that their object of study is physical and embodied;
however the Enactive approach, along with other non-representational approaches, proposes an explanation that draws deeply on the embodiedness and situatedness of the processes:
it creates notions of cognition and meaning that are defined in terms of the coupling of physical systems, rather than in terms of symbolic processing.
The initial formulation by @varela_embodied_1991 led to many developments.
We present here the main theory going under the name "Enactive approach", and do so in four important conceptual stages which we believe roughly (though drastically) summarise what has been developed by @torrance_search_2006, @thompson_mind_2007, @de_jaegher_participatory_2007, and @cuffari_participatory_2015.
While this will by no means do justice to the complete approach, we hope these stages will provide a clear-enough sketch of the dynamical account of cognition that the Enactive approach proposes.


#### Stage 1: sensorimotor contingencies

The first stage is a reconceptualisation of the way an organism perceives its environment.
This view, known as the sensorimotor approach to perception [@oregan_sensorimotor_2001], construes perception as an exploratory activity based on a continuous perception-action loop.
By contrast, the default approach to perception is to construe it as an inference problem:
through its senses, an organism receives information about the world and attempts to reconstruct an internal representation of it, which is challenging because the information is degraded in a number of ways.
Instead, the sensorimotor approach construes perception as the exploration of the regularities in the way stimulations change when the organism moves around or acts on its environment or on an object.
Under the sensorimotor account, an organism that is perceiving an object is not inferring and internally representing its properties, rather, it is exploring the changes it can generate in sensory stimulations by moving around or interacting with the object.
The percept of an object is then precisely the regularities that the object creates in the perception-action loop.
As @oregan_sensorimotor_2001 put it:
"seeing constitutes the ability to actively modify sensory impressions in certain law-obeying ways."
An extreme example of such actively perceived properties is the softness of a sponge, which is felt by prodding and squeezing it but not through static contact [@myin_account_2003].

A strong motivation for this approach is that, when applied to a perceptual modality as a whole (vs. concentrating on a particular object), it provides an endogenous account of the feel of that perceptual modality (also called its *perceptual consciousness*).
This has been a longstanding problem in inferential approaches to perception.
According to the sensorimotor approach, seeing and hearing feel differently (i.e. one can easily differentiate visual from auditory consciousness) not because they are processed by different parts of the brain, but because of the specific regularities with which stimulations are deformed in each sensory modality when the organism moves.
Turning our head, for instance, generates a certain change of stimulation in vision, and a different change in hearing.
For a given modality, the relationship between an organism's movement and the way stimulations change because of that movement is called the *sensorimotor contingencies* of the modality;
these characterise the type of perceptual consciousness the modality creates.

An interesting confirmation of this prediction is found in experiments using "Tactile Visual Substitution systems", where blind people are equipped with a device that reproduces on their skin (through an array of stimulators) the luminance patterns captured by a camera, and are then tested on their ability to recognise objects using this cutaneous stimulation.
Crucially, subjects are not able to recognise objects when the camera is static, but are able to do so when they actively control the movements of the camera [@oregan_sensorimotor_2001, p. 958].
Furthermore, once they do control the camera, their sensations seem relatively close to actually seeing:
they begin to perceive objects as not on their skin but in front of them;
they can be frightened by a zooming effect in the stimulations (which would encode an object that is approaching very fast);
the location of the stimulator array on the body becomes unimportant, as subjects can easily transfer from stimulation on the back to stimulations on the forehead.
The sensorimotor account predicts these effects, explaining that the similarities with vision come from the fact that the Tactile Visual Substitution system creates sensorimotor contingencies that are very close to those of vision itself.
This experimental approach is generalisable to other perceptual modalities, and supports the idea that perception and action are two sides of the same dynamical interaction loop with the environment, an interaction loop whose regularities define the way a particular object is perceived, and the way a given modality feels overall (its perceptual consciousness).


#### Stage 2: sense-making

The second stage extends this approach to all of cognition.
In a nutshell, it can be seen as taking the shift operated by the sensorimotor approach and applying it to meaning and cognition.
The sensorimotor approach departs from a definition of perception as the inference of external information present in the world, and suggests instead that to perceive an object is to feel (and explore) the regularities in the way stimulations change in a perception-action loop.
Similarly, instead of seeing meaning as some information that must be inferred from an interaction, the Enactive approach proposes that meaning is something that is primarily felt (vs. inferred), and defined by the regularities of the dynamical interaction of an organism with its environment [@de_jaegher_participatory_2007;@thompson_mind_2007].

To operate this reconceptualisation of cognition and meaning, the Enactive approach proposes a new definition of cognition, which is tied to a property of life itself.
Inspired by the notion of autopoiesis developed by @maturana_autopoiesis_1980, it considers any living organism as an *autonomous system*, that is a network of processes that have the following properties:

1. The system is self-produced and self-maintained.
   The processes continually re-produce themselves as whole, and depend on each other for continued operation.
   In particular, the network has *operational closure*:
   every process in the network is conditioned on the activity of one or several other processes of the network.
   The very existence of this operational closure defines an identity for the network of processes.
2. The system continually produces a boundary that distinguishes it from the environment (this need not be a physical boundary).
3. The system actively regulates its interaction with the environment in order to maintain its identity.

Crucially, the identity generated by operational closure is precarious:
it disappears if some or all of the processes that make up the system cease.
The system is thus in a permanent tension to regenerate the conditions for the continuation of its identity, and any interaction with the environment thus acquires an inherent value to the system since it can have positive or negative consequences on the continuity of the system's identity and autonomy.
Since interactions with the environment are necessary for the network of processes to keep self-generating, the system is continuously regulating the strength of its coupling with the environment in order to maintain its identity.
Interaction with the environment then becomes inherently meaningful to the system, and the Enactive approach calls it "sense-making".

![Sense-making:
an autonomous system self-produces both itself and its separation from the environment (circling arrow);
it is coupled to its environment (full arrows), and actively regulates the coupling in order to maintain its identity (dotted arrows).
Reproduced from @froese_enactive_2011
(©2010 E. Di Paolo;
Licensed under Creative Commons Attribution 3.0 Unported [\url{http://creativecommons.org/licences/by/3.0}]).
](images/discussion/froese_enactive_2011-sense_making.jpg)

In this framework, cognition *is* precisely the sense-making activity, that is a system's actively regulating its coupling to the environment in order to maintain its identity.
Notice how the enactive notion of meaning is defined in a parallel manner to the sensorimotor account of perceptual consciousness:
instead of being a piece of information that is inferred and represented, it is something that is felt as a property of the dynamics of the system's interaction with its environment.


#### Stage 3: participatory sense-making

The third stage extends the idea of sense-making to the interaction of two autonomous systems.
In doing so, this stage also identifies a fundamental tension that appears in all interactions:
interactions themselves can acquire a level of autonomy and regularity that is beyond the control of individual participants, and can constrain their possibilities for individual behaviour.
This tension will serve as the basis for structuring interactions in the next subsection.

@de_jaegher_participatory_2007 develop this stage in two steps.
First, they argue that some interactions can only be explained at the level of the interaction itself, rather than at the level of participants.
An interesting point in that direction has been made in an experiment by @auvray_perceptual_2009.
The experiment involves two subjects who share a virtual line on which they each have a cursor.
The line and cursors are all invisible, but the subjects receive haptic feedback whenever their cursor is overlapping with the other's cursor.
Aside from the subject's cursors, two fixed obstacles are placed on the line (each is perceivable by one subject and not the other), and the cursor of each subject has a shadow that follows it at a fixed distance:
when subject A's cursor touches the shadow of subject B's cursor, subject A receives haptic feedback but subject B does not (and vice-versa).
The subjects are told about obstacles but not about shadows, and are tasked with clicking as much as possible on each other's cursors.
Interestingly, they succeed in doing so, but not because they are able to distinguish between real cursor and shadow.
The experiment shows instead that they are not able to make the distinction individually, but solve the task because the interaction of real cursors is more stable (and thus more frequent) than the interaction with a shadow:
subjects individually fail the task while succeeding collectively, in a way that can only by understood because of the inherent (and unnoticed) stability of their interaction.
The principle highlighted by this experiment is that of the stability of *perceptual crossings*:
two organisms can have a dynamically stable interaction because they each look for a behaviour that they themselves create, without necessarily being aware of that fact (for instance mutual gaze of an infant and his mother, where the infant may not be aware that his mother maintains the gaze because he does too).
Variations and detailed behaviours in this experimental paradigm have been extensively explored in this literature, providing further support for the results above [see for instance @bedia_quantifying_2014; @froese_embodied_2014].

![Perceptual crossing setup introduced by @auvray_perceptual_2009.
Each participant controls a cursor on a shared virtual line (red squares), to which a sensor is attached (black ticks next to the red squares).
The movements of each participant's cursor are mirrored by a shadow cursor (blue squares).
Each participant additionally has a fixed obstacle on the line (one obstacle for each participant, black squares).
Participants do not see the line or cursors, but receive haptic feedback when their sensor touches the obstacle, the other participant's cursor, or its shadow.
Reproduced from @de_jaegher_can_2010
(©2010 H. De Jaegher, E. Di Paolo and S. Gallagher;
Licenced under Creative Commons Attribution 3.0 Unported [\url{http://creativecommons.org/licences/by/3.0}]).
](images/discussion/de_jaegher_can_2010-perceptual_crossing.jpg)

Second, @de_jaegher_participatory_2007 argue that such stable interactions can acquire an autonomy of their own.
An example that most people have experienced in everyday life usefully illustrates their point:
when trying to cross someone else in the corridor of a train, and moving to the side to avoid them, at times the other person spontaneously moves to the same side you did;
when this happens, you and the other person enter an interaction which both are trying to break from the start:
each one moves to one side, and the other does the same, until your movements desynchronise and the interaction breaks down.
During the time it persisted however, the interaction acquired its own autonomy which constrained both you and the other person, as neither could break free from it.

This autonomy serves as the basis for defining sense-making at the level of the interaction itself:
when two organisms interact while at the same time regulating their coupling to their environment (and respecting each other's autonomy), the interaction itself can spontaneously self-organise and become self-sustaining.
Similarly to organisms, then, it acquires an identity of its own, and an interest in maintaining that identity:
in that case, since the couplings of each organism to their environment and with each other have an impact on the continuation of the interaction, they become meaningful *to the interaction* which can then partly regulate them.
A new sense-making activity thus appears at the level of the interaction itself, a level that neither of the participants fully control, and which has the potential to create constraints on them.
This act, where two participants co-regulate an interaction that is beyond complete individual control, is termed *participatory sense-making* [@de_jaegher_participatory_2007].
It has become a key building block of the enactive theory of interaction as it provides a well-founded and naturalised path to explaining emergent effects in interactions.
It is also the foundation on which the enactive account of language is constructed, as we will now see in the fourth and final stage.

![Participatory sense-making:
each organism regulates its coupling with the environment, but the two regulations become themselves coupled such that the interaction between the two organisms acquires a level of autonomy of its own.
Reproduced from @froese_enactive_2011
(©2010 E. Di Paolo;
Licensed under Creative Commons Attribution 3.0 Unported [\url{http://creativecommons.org/licences/by/3.0}]).
](images/discussion/froese_enactive_2011-participatory_sense_making.jpg)


#### Stage 4: languaging

Relying on the concepts defined above, @cuffari_participatory_2015 propose to see language as a specially structured pattern of participatory sense-making, governed by several levels of conventions interlocked with one another.
More precisely, since a crucial feature of participatory sense-making is that the interaction itself acquires regularities that neither participant controls, it follows that interactions create some sort of tension between the way individual organisms regulate their autonomy and the regularities that the interaction may impose on them if they are to continue interacting.
Consider once again the perceptual crossing experiment introduced above.
In the initial setup by @auvray_perceptual_2009, subjects are not able to distinguish between the other's cursor and its shadow, such that prosociality is neither presupposed nor observed and yet the pair collectively succeeds in accomplishing the task (clicking more on the other's real cursor than on any other object on the shared line).
They succeed because the interaction that appears when the two cursors cross each other is naturally stable:
when the subjects cross each other, both are informed by haptic feedback, such that both come back on their steps to explore the object they just touched.
The interaction thus leads both cursors to criss-cross each other for a small period of time, until one of them moves a bit too far and the stability breaks down;
this kind of behaviour does not appear when a cursor touches a shadow, as in that case one of the two subjects is not informed of the encounter.
Thus the stability of the interaction results from the way the spontaneous actions of the two subjects dynamically interlock and become coupled;
this is a regularity at the level of the interaction that participants do not control, and do not even detect (recall that they fail to individually distinguish between the other's real cursor and their shadow).

Now suppose that subjects can be sensitive to that regularity:
a tension appears between the naturally occurring stability, on one side, and the way subjects would like to act.
Indeed, subjects become able to identify when they are in the course of a stable interaction and when they are not, but have a priori no way of influencing that interaction without breaking it, since its very existence relies on the naive behaviour described above:
the only way to interact is by following the naive rules.
@froese_embodied_2014 created exactly that situation with two small changes to the perceptual crossing experiment:
first, they allowed each participant a single click per session, making them much more conservative in their behaviours;
second, they framed the experiment as a cooperative task where subjects should help each other in detecting each other's cursor (subjects are still not aware of the behaviour of the shadow cursor).
In this situation, subjects become sensitive to interactive stabilities, and most importantly they manage to resolve the tension described above.
Instead of both cursors permanently criss-crossing each other, a kind of turn-taking behaviour spontaneously appears where one subject stays still while the other criss-crosses it, then the roles are reversed and the first one criss-crosses the second one that is now staying still.
Thus a new level of interactive regularity appears, built on the previous one:
turn-taking in the perceptual crossing.
^[Interestingly, the authors also ask the subjects to give Perceptual Awareness Scale ratings for the moments at which they click, and find that such turn-taking episodes correspond to a mutually heightened perceptual awareness of the presence of an other.
]

@cuffari_participatory_2015 generalise and recursively expand this kind of emergence of a higher-order interactive norm.
The level we just described corresponds to the emergence of what they call *co-defined social acts*.
Co-defined social acts are like salutations, or acts of giving and receiving:
they cannot be completed by one person alone.
One person can initiate a social act with a *partial act* (e.g. extending a tentative hand to be shaken in the case of a salutation, or holding out your keys to the person you want to give them to), but the other person must appropriately react to that initiation in order to complete the act (grasping the extended hand and shaking both together, for the case of salutation, or taking the keys offered in the case of giving and receiving).
Otherwise the act fails and the interaction breaks down.
There is thus a new tension at this level:
co-defined social acts can only be completed, or sustained, if both participants know how to behave when faced with a partial act.
In the case of giving and receiving, if the person you are extending your keys to does not reach out to receive them, the act fails.
In the case of perceptual crossing, this corresponds to one participant criss-crossing the other's cursor, and the other participant not responding with the same act;
in such a case, turn-taking would not appear and the interaction would break down.

Now, if in this situation participants become sensitive to the fact that a partial act is not being completed adequately, and if they have other social acts at their disposal, yet another level of
interactive norm can develop:
some social acts can be used to regulate other social acts (e.g. ostensively staring at your own extended hand to signal to your interlocutor that they should shake it).
This new level of interactive norm then leads to its own tensions, which are again resolved by yet another higher interactive norm.
The expansion thus continues by building each level of normativity as the resolution of a tension between the types of individual and interactive autonomies that exist at the previous level.
@cuffari_participatory_2015 propose 8 levels of normativity,
^[The levels are Participatory sense-making (which we started with), Social Agency (e.g. turn-taking in perceptual crossing), Coordination of Social Acts (e.g. giving and receiving), Normativity of Social Acts, Community of Interactors, Mutual Recognition and Dialogical Structure, Participation Genres, and finally Languaging.
]
each one corresponding to a new sensitivity of the interacting organisms to a regularity or constraint at the previous level.
Often, the new regularity and its regulation by the participants appears at a different time scale, or in a different dimension than at the previous level.
The authors thus propose that linguistic interactions (or languaging in enactive terms) can be understood through a gradual progression of interactive norms, tensions, and resolution by new norms, where each step lays more of the groundwork necessary to construct full linguistic behaviour [words, for instance, then appear as "patterns available for enacting certain forms of sense-making", @cuffari_participatory_2015 p. 32].
To our knowledge, the higher levels of this expansion have not yet been empirically validated in the manner described above for the first two levels.
Indeed, as one progresses in the levels of normativity, the difficulty in creating simple interactive situations that can validate this proposal grows extremely.
Still, the theory proposes a roadmap for making progress towards an explanation of language, meaning, and linguistic interactions which is fully grounded in the dynamics of interaction between participants.
In such an account, linguistic meaning would appear as a particular structure of the interactive dynamics that is felt by its participants, and linguistic structure itself would result from the recursive interactive norms that are developed to resolve tensions in interacting communities.
Here too, there is a parallel with the sensorimotor approach to perception:
similarly to the way perception is seen as an exploration of the regularities of perception-action loops, the Enactive approach proposes an account of meaning itself as a regularity that is felt in social interaction, and structured by the contingencies of the interaction itself.


#### Discussion

A number of points can be noted about the approach we just outlined.
First, it reflects the intuitive idea hinted to in our previous discussion of meaning changes in transmission chains (@sec:discussion-empirical-results):
in meaningful interactions, "everything matters".
More precisely, anything *can* matter:
any seemingly minor detail of the dynamics of an interaction may (or may not) become extremely important if for some reason the participants are sensitive to it in one way or another, and rely on it for instance to resolve a tension.
In particular, when starting from a higher level of description (such as symbolic processing in Relevance Theory), one can neglect aspects of meaningful interactions which turn out to be essential ingredients.
A reconstruction that starts from both simpler interactions and simpler levels of description, such as the one encouraged by the enactive standpoint, is more likely to pick up on such ingredients.
As noted previously, the problem here is the extreme complexity of interactions and contextual situations, which the Enactive approach tackles by starting from simpler (but, crucially, always meaningful) interactions, and by using the language and tools of dynamical systems theory [see for instance @beer_dynamical_2000;-@beer_dynamical_2014].
A second point related to the "anything can matter" intuition is that the coordination of interacting organisms that is necessary to achieve dynamical coupling can rely on a diversity of dimensions:
while interacting organisms need to have comparable dynamics to make it possible for an actual coupling to emerge in their interaction, different dimensions of the dynamics are eligible to that role at different levels of interactive normativity.
For instance, coupling in perceptual crossing experiments is likely to emerge only when the subjects make movements of comparable magnitude at the sub-second timescale.
Turn-taking on the other hand, is likely to require the subjects to have comparable behaviour at the timescale of a few seconds (the duration of a turn in turn-taking).
The poorer the match in dynamics in a given dimension at a given scale, the more difficult it should be for a coupling to appear in that dimension at that scale.
Higher levels of interactive normativity will likely involve yet larger timescales [a point that could be related to multi-scale complexity matching in conversations, @abney_complexity_2014].

@dale_self-organization_2014 provide a review of the empirical work that has already been done in (not exclusively enactive) dynamical approaches to interaction.
At the level of simple interactions, promising applications have been made with software implementations:
@botelho_software_2015, for instance, develop software agents endowed with inherent goals, such that they could have a very simple but endogenous notion of meaning;
@froese_dynamically_2013 further illustrate the dynamical coupling of maximally simple software agents that manage to collectively solve a task precisely (and only) thanks to their coupling (similar to what the perceptual crossing paradigm explores with human subjects).
Now as regards language itself, the approach evidently needs to be much further developed and empirically explored.
The problem seems tractable however, as the mysterious aspects of meaningful interaction are already part of the simpler levels of interaction that the current theory convincingly accounts for.

Finally, let us note that the general approach of expanding normativity levels on top of the previous levels provides a natural continuity between language and other social interactions broadly construed, a property that is relevant when comparing meanings and interactions across languages.
Any speaker who is fluent enough in more than one language has experienced the change in the way social interactions feel and unfold when switching from one language to another (the effect is strongest when using different languages with the same interlocutor).
English and Spanish, for instance, afford different types of interactions:
each language constrains meanings differently than the other;
one language will implicitly express a particular set of connotations that the other does not, such that it is necessary to spell out those connotations explicitly when translating one language into the other.
In the words of @evans_your_2011, who studies this link between a language and social interaction:
"Languages differ not so much in what you can say as in what you must say" [-@evans_your_2011 p. 70].
One could go as far as saying that speaking another language is not so much a different encoding or a different set of rules, it is a different way of being.
Now on the enactive account, the grammar of a particular language as it is classically understood (i.e. structured utterances with phonetics, phonology, morphology and syntax in English or Spanish) could correspond to a particular expansion of normativity levels that differs from that of another language.
In particular, if the grammar-related normativity levels differ, the levels above and below will also, a fact that could correspond to the different types of agency and interaction afforded by different languages.
One can thus expect the different feel of different languages to be explicable through such a continuity of language and agency, once the theory has been further fleshed out.


### Applied to cultural attraction

Compared to Relevance Theory, the Enactive approach starts from radically simpler types of interaction, and relies neither on a capacity for symbolic processing nor on internal representations of the environment.
This makes the approach slightly more involved to present than Relevance Theory.
In spite of their differences, both approaches build on a notion of relevance or value to the individual, and use it to ground a notion of meaning (be it linguistic or not).
Unlike RT, the enactive notion of meaning exists even in the most basic types of interaction an organism can have with its environment.

The two approaches can be seen as starting from different descriptive levels and building what is missing for an account of meaning (though this characterisation does not exhaust their differences):
RT starts with representations and symbolic processing, which are implementable in bodies (by following the model of computers) and come already structured in similar ways to language, and constructs on top of those a notion of meaning, with a corresponding interpretation procedure (inference), that is subtle and flexible enough to apply to fuzzy cases like poetry or loose language.
The Enactive approach starts from an embodied and dynamic notion of meaning, which by definition exists for all organisms and need not be inferred, but comes without any particular structure;
it then builds its account of language by detailing how interactions (and thus meaning) can become increasingly structured through a series of emerging levels of interactive normativity.
^[In doing so, it also provides a partial account of the phenomenology of meaning, something that RT can only provide through a definition of the exact content of representations and a mechanism for how they represent that content.
For more details, see again @hutto_radicalizing_2013 on the Hard Problem of Content.
@harvey_content_2015 also provides a useful discussion of that question.
]
Unlike RT, it does not yet provide a clear way to compare meanings across interactive situations, as its very definition of meaning is intrinsically related to the subtle differences of dynamics in different interactions.
In RT, that work is theoretically done by the notion of representation (or more precisely, the informative intention that is inferred and represented), although in practice much remains to be defined to directly compare representations between each other.
In its current state, the Enactive approach is applicable to much simpler interactions than those tackled by RT, as more experimental work is needed to confirm the higher-level parts of the theory.

This finally brings us back to the usefulness of both these approaches for CAT.
First, let us note that @sperber_explaining_1996 refers repeatedly to RT as an important tool to understand the role of interpretation in the epidemiology of representations.
Indeed, while he does not flesh out the connection in more detail, the information-processing account provided by RT fits well with the representational foundation of CAT and of our own experimental approach.
But the fact that CAT is formulated in terms of representations is not necessarily a theoretical constraint to integrate it with an enactive approach to meaning:
similarly to what we argued above for RT, information-processing can be seen as no more than a particular descriptive starting point, with no strong import for the ontology of what is being transmitted or evolved,
^[Indeed, @sperber_explaining_1996 [p. 135, footnote 40] explains that his view of the content of concepts (which we can assume to be close enough to the content of a representation) is influenced by @millikan_language_1984, which is not so far from non-representational views of content [@harvey_content_2015].
]
and we see no reason to consider a priori that the two approaches would be incompatible.

In current practice however, the fact that CAT gives so much autonomy to a contentful notion of representation encourages empirical approaches to use a code model and give context a minimal role (i.e. a representation is an encoding of some content).
Most applications thus strip down the context-dependence of CAT, thereby obviating the need for Relevance Theory in the first place.
By not seeing that the contents of a representation are inherently contextual and constructed in interaction, one reduces the cultural evolution process to a simple accumulation of interpretations that do not rely on context:
such a simple model depicts representations as being straightforwardly interpreted into mental versions (with some degree of transformation through reconstruction) then produced anew as public versions (again with some degree of transformation), and the circulation and tranformation of representations creates a dynamical system with attractors.
As we just saw, both the Enactive approach and RT show that taking context into account drastically changes the picture.
The fact that it is possible to easily degrade CAT into a code model theory also suggests that integrating it with an approach to meaning might require some amendments.
We thus see great value in contrasting and possibly combining the two approaches to replace the naive code model that is used in many applications of CAT.
In practice, attempting to apply these two approaches will also let us explore if the theory must be amended in order to fully take the situatedness of meaning into account.
^[Anthropologist Tim Ingold, for instance, has argued at length that the whole project of a Darwin-inspired theory of cultural change can only make sense if one accepts a misguided dualism between context-free information on one side and material encoding of the information on the other, a dualism that is encouraged by the computational metaphor of cognition [@ingold_complementarity_1998;-@ingold_transmission_2001;-@ingold_beyond_2004;-@ingold_trouble_2007].
The requirement of being able to compare meanings across situations (so as to analyse their evolution), in particular, would rest on being able to identify information that exists independently of the situation in which it emerged.
The alternative he proposes is heavily inspired by dynamical approaches to cognition (such as the Enactive approach) and a developmental focus on biology and heredity.
While we agree that computational approaches to cognition are prone to such dualism, information-processing analyses can also be used as a tool to combine with dynamical approaches, without taking the computational metaphor too seriously.
Moreover, we see no reason to believe that dynamical approaches could not be compatible with Darwin-inspired approaches to cultural change.
The question of how to relate meanings from different interactive situations is, we believe, better tackled through further formalisation and empirical investigation of the available accounts of meaning.
]


## Empirical speculations

Following @chemero_after_2008 and @beer_information_2015, we believe that contrasting and combining practical uses of the information-processing and dynamical approaches is the best way to move forward in debates that otherwise turn into scholasticism.
This section proposes a few avenues to do so in the context of meaning-related cultural evolution.
The first point we discuss is an additional method that is directly applicable to the previous chapter's data.
The second and third points discuss approaches similar to the paradigm presented in the previous chapter, where ecological material is used and the full complexity of language must be dealt with.
The final point discusses an approach similar to the perceptual crossing paradigm, which uses much simpler interaction situations, creating behaviours of lower dimensionality where it is easier to understand complete dynamical patterns.


### Hand-coded meaning classes

Our first point is an analysis method that combines manual and computational steps.
It is directly applicable to the data sets of the previous chapter without necessarily adding a notion of context, and is also usable for analysing the data that would be generated by the approaches we delineate further down.

The first step is to devise a manual measure of the similarity between random pairs of utterances in a tree;
given such a manual measure, a dimensionality reduction method can be used to represent utterances in a 2-dimensional space.
Plotting the parent-child links between utterances will then provide a 2D representation of the evolution of meanings in the tree.
On such a plot, one can further identify classes of closely-related meanings in the tree (an operation that can also be done through clustering on the similarity measures), and attribute each utterance of the tree to a particular meaning class.
These techniques make the evolution of meaning much more manageable, as each utterance is then assigned to one of a finite number of meaning classes, and each transformation represents a transition from one class to another.
The transition probabilities between meaning classes also give an idea of the types of transformations that happen most often at the level of meanings, as encoded by the similarity measure.

The main challenge for this method is the first step, that is gathering manual measures of similarity between utterances in a tree.
One reliable approach to this problem is to gather similarity ratings in a triad comparison task [@romney_culture_1996]:
manual coders are presented with three utterances, and must decide which of the three is most different in meaning to the other two (in which case the other two are considered to have some degree of similarity).
While the complete set of triad comparisons grows as $n^3$ ($n$ being the number of utterances in a tree) and is thus too expensive for non-trivial trees (such as in Experiment 3 in the previous chapter, where trees contain 71 utterances), it is not necessary to rate all the combinations in order to obtain a reliable picture of meaning classes in the tree.
Indeed, in the complete set of triad comparisons, each pair of utterances appears $n-2$ times.
The so-called Balanced-Incomplete Block Design [@weller_systematic_1988 pp. 49-55] compares triads without repeating pairs more than a fixed number of times, in which case the number of triads grows as $n^2$.
^[For $n = 71$, as in Experiment 3 of the previous chapter, there are no Balanced-Incomplete Block Designs where each pair of utterances appears exactly once.
However, designs exist for $n = 69$ and $n = 73$, and can be created from trees with 71 utterances by removing two random utterances or adding two dummy utterances.
$n = 69$ yields 805 triad comparisons, and $n = 73$ yields 876 [see @weller_systematic_1988 for the detailed computation].
More reliable results can be obtained by using designs where each pair appears exactly twice, which doubles the number of triad comparisons.
]
Triad comparisons can be obtained in large quantities by outsourcing the task on platforms such as Amazon Mechanical Turk, or can alternatively be integrated in the online transmission chain platform.
^[Consider for instance an experiment similar to Experiment 3 of the previous chapter, with 71 utterances per tree (one root, and 7 branches of 10 reformulations), and a targeted 70 subjects in total.
In a given tree, the number of triad comparisons with each pair appearing exactly once is at most 876 (see previous footnote), which can be distributed as an average 12.51 comparisons per subject.
Counting about 20 seconds per comparison, the added comparisons thus require each subject to spend, on average, an additional 4m10s on each tree.
Given that individual transformations took an average 1m12s in Experiment 3, that is subjects spend on average 1m12s on each tree in Experiment 3, this conservative estimate multiplies the total cost of the experiment by about 4.5.
Collecting these triad comparisons for all the trees is thus quite costly, but doing it for one or two trees only is very affordable.
]

The modified Correspondence Analysis used by @romney_culture_1996 on this type of data yields an $n$-dimensional representation of the utterances, where the first dimension distinguishes the utterances the most, and the last dimension the least.
Utterances can then be plotted on the first two dimensions, grouped into classes by a clustering method, and the evolution along the branches thus reduced to transitions between meaning classes.
Such a measure would provide an interesting first insight into the meaningful transformations operated by the subjects.


### Minimal interaction and context

The transmission chain paradigm can also be tweaked in at least two ways to begin exploring the role of context and interaction.
One area to explore is to consider utterances not in isolation, but as part of a contextual paragraph.
The surrounding paragraph could provide enough background for subjects to have a feel of what it amounts to pronounce that utterance in context.
The task can then be framed as role-playing:
subjects would be asked to imagine the scene depicted by the paragraph they just read, imagine themselves as the person pronouncing the last utterance (or an utterance in the middle, highlighted as they were reading), and rewrite it.
The larger the context, the more constrained we expect interpretation to be, thus the more limited we expect transformations to be.

A second change, which can be combined with the first, is to introduce minimal forms of live communication in the task so as to embed the subjects in an actual communicative task.
For instance, the transmission step could be turned into a minimal interaction where the first subject proposes an utterance (that is, their memorisation of what they read) and the second can either accept it or ask for a better reformulation (up to a maximum number of times), in which case the first subject must rewrite their proposal.
The interaction requires a fine balance of bonuses and rejection penalties so that the functionality is not abused, but it gives subjects an interest in the task:
a receiving subject is encouraged to ask for a consistent utterance, because they will later be asked to pass on their own memorisation of that utterance to another subject.
Combining such minimal interactions with contextual paragraphs could further ensure that subjects do not change to a completely different utterance in order to see their proposal accepted:
if the receiver is presented with the contextual paragraph when deciding to accept or reject the proposed utterance, then they will expect the proposal to be consistent with its context.
The possibility for multiple proposals introduces a new challenge however:
if, for instance, a receiving subject rejects two proposals and accepts the third, it is not clear which of the rejected or accepted proposals they will base themselves on when writing their own proposals for the next subject.
Parent-child links thus become less well-defined in this situation.

While these changes may allow us to introduce minimal forms of context and interaction in a transmission chain, the paradigm still dissociates the complexity of one communication means, namely the utterances (which are realistic and thus have highly complex linguistic structure), from the complexity of another communication means, namely the interaction itself (which is minimal).
This is quite compatible with Relevance Theory, but the result is still somewhat unecological:
if context and interactive situation are crucial to the way meaning is understood (according to both Relevance Theory and the Enactive approach), it would be more natural to match the complexity allowed by the interactive situation with the complexity of other communication means that are provided to the subjects.
At the very least, it seems necessary to have an interactive situation at least as versatile as the productions that are asked of the subjects.
In other words, asking subjects to write complex utterances while constraining their non-verbal interaction to a binary acceptance-rejection outcome still only makes sense in a code model theory, where complex meanings can be understood in spite of the interactive situation being extremely simple.
To create situations where the meshing of context, interaction and meaning can be studied, then, it is necessary to match the complexity of all the communicative means provided to the subjects (interactive, verbal, or other).
In what follows we discuss possibilities for analysing situations where the complexity is close to that of real life.
We then move on to situations where both the interaction situation and the possible meanings have a much reduced complexity.


### Fully measured contexts

In some cases, it is possible to fully measure all observables in an unconstrained interactive situation.
The first way of doing this is to have enough sensors in an *in vivo* situation:
the Human Speechome Project [@roy_human_2006], for instance, equips a family's house with wide-angle ceiling cameras and microphones that map the entire space of the house, and records the quasi-totality of what happens in the first three years of the life of a newborn in the family (with some privacy controls).
Everything the child hears and sees, all the interactions she is involved in are taped in a way that makes it possible to reconstitute the detailed movements, vocal productions, and relevant interactive features of participants (such as gaze).
The analysis of such rich data is humongous and involves many novel semi-automated coding techniques (as the dozens of data-analysis publications related to the project^[See \url{https://www.media.mit.edu/cogmac/projects/hsp.html} for a full list.] attest to), but gives access to the full detail of interactions that a child is exposed to in her house.
@roy_predicting_2015, for instance, use the data to explore the spatial and linguistic contexts in which a child is exposed to a word, and relate them to the context in which she first produces said word.
Such longitudinal multi-modal data is exactly the type of measurement that is necessary to fully understand the meaning that is produced in an interaction, explore when that meaning is reproduced, and see how it changes through time.

The second way is to create an *in vitro* interactive situation that is as encompassing as possible, and measure every possible aspect of the interaction in it.
In the laboratory, this corresponds to the approach taken by @moussaid_amplification_2015, who taped unconstrained pairwise conversations chained one after the other, hand-coded them for specific features, and then measured the evolution of such features along chains.
Another approach consists in taking advantage of online video games that create complete worlds in which the interactions of players can be measured in multiple modalities (gaming behaviours and parallel verbal conversations), as has been used for instance in the study of language learning [@zheng_multimodal_2012].
However, developing such a game for experimental purposes involves a substantial investment, such that researchers must often rely on existing gaming environments and communities to access the data.
^[Smartphone psychology [@miller_smartphone_2012] might be another opening for this type of study:
by getting a closed group of people to accept that all the measurable aspects of their interactions be recorded by their smartphone for a period of time (sound, surrounding luminosity, small movements of the device, precise location of the subject), an experimenter could potentially have access to a much richer view of interactions of everyday life.
This does not seem possible for fully unconstrained interactions however, and must likely be restricted to a particular area such as work or home;
otherwise, the consent of every person that a subject interacts with in their everyday life would have to be obtained.
]

Fully measuring unconstrained interactions is thus one option for matching the complexity of the different communicative means available to the subjects.
The challenge, however, is precisely the increased complexity of context and interaction:
for cultural evolution, the works we reviewed in this and previous chapters use important manual or semi-automatic steps in their analyses, such that their access to meaning in the data is in large part interpretive.
Still, while using the relevance-theoretic approach to study how meaning is inferred in such situations can quickly become unmanageable, it may be possible to gradually increase possibilities by developing better semi-automatic coding techniques, similarly to what has been done for the Human Speechome Project.

We now turn to the other end of the spectrum:
approaches which, by relying on the Enactive account for which simpler meanings can exist without the requirement of symbolic processing, aim to lower the complexity of both interactions and possible meanings.


### Preliminary enactive steps to language

This final approach aims to take advantage of the empirical appeal of the enactive notion of meaning:
using paradigms similar to that of perceptual crossing, one might be able to create interactions that have meaning to the participants without the need to use complex linguistic material.
In such a situation, participants would interact through a combination of low-dimensional channels in a task that has inherent meaning to them, and both the interaction and the meaning emerging from it would have extremely low complexity.
Such an approach would not enable participants to have everyday life conversations, but it would let the experimenter introduce complexity in experimental tasks little by little, with the end goal of creating situations whose complexity can approach that of actual linguistic interactions.

In its current state however, the Enactive approach does not yet provide a full account of language.
The phenomena it does account for, such as sensorimotor contingencies and perceptual crossings, are better described as potential preliminary steps to language:
they are elements that can be combined and fleshed out in order to develop an explanation of language.

The theory of languaging developed by @cuffari_participatory_2015 proposes a way to do so:
it progresses in a series of levels of interactive normativity, each one appearing as a resolution of a tension at the previous level.
The first normativity level is that of participatory sense-making, which the perceptual crossing stability illustrates [@auvray_perceptual_2009]:
two participants are coupled in an interaction that neither of them controls, or even detects.
The second normativity level is that of Social Agency, illustrated by the turn-taking phenomenon that appears in perceptual crossing when participants are sensitive to an interactive stability [@froese_embodied_2014].
As we mentioned earlier, this phenomenon can be seen as a co-defined social act, that is an act that is initiated by one person (who makes a partial act), and must be completed by the other person recognising it and responding appropriately (e.g. giving and receiving).
The authors propose another 6 conceptual stages which together would provide a first account of the languaging behaviour of human beings.
While these stages are proposed as conceptual steps, and not as causal steps that would correspond to the actual progression of communicative complexities found in other species or through evolution, an important milestone here would be to empirically validate the remaining six stages in a similar fashion to the first two.
If successful, completing such a validation would provide a more comprehensive view of the enactive language account, and would open the way for integrations with cultural evolution theories.

The third normativity level, for instance, that of Coordination of Social Acts, emerges when one social act is used to regulate another social act.
The authors' example of this normativity level is a Japanese woman extending her personal card to a westerner.
The woman offers her card by using the partial act that is adequate in Japan:
holding it out with *both* hands.
The adequate completion of this partial act in Japan is to accept the card with both hands too.
However, a westerner would spontaneously take the card with only one hand, a response that a Japanese would not consider appropriate (indeed, it amounts to answering with part of one's body to a partial act that involves the whole body).
In response to this, the Japanese woman could hold on to her card, and ostensively stare at the westerner's other hand, thus initiating a second partial act to regulate the first.
The westerner, understanding the regulatory stare, could nod (thus completing the regulatory act) and accept the card with both hands (thus completing the initial act of giving).

This type of regulatory social act can appear when a second channel and a repertoire of acts are available to regulate what happens in a primary channel.
To empirically validate this normativity level, then, one could imagine setting up two parallel perceptual crossing tasks:
the first, more complex, would involve cursors on a shared surface instead of a line;
the second, simpler, would be the classical uni-dimensional perceptual crossing setup.
We reason that the simpler channel could emerge as a regulatory aid for a new type of social act to emerge in the more complex channel.
This simple extension of perceptual crossing is most likely not the best, but it illustrates the type of experiment that one could devise in order to observe the emergence of the third stage.

In the process of validating the third normativity level, several conventions will appear that depend on the history of interactions between agents and in a community.
For instance, one repertoire of social acts will develop in one community of frequently-interacting participants, and a second repertoire will develop in another.
Participants from different communities that are brought together will be forced to adjust, expand, or develop anew their repertoire of social acts.
This setting opens a number of questions about the evolution of such minimal social acts in communities.
In order to explore the integration of this approach with CAT, it will be necessary to empirically flesh out the full range of normativity levels in this manner.
While the task is assuredly challenging, we believe that such a project has great potential to possibly complement a relevance-theoretic approach to meaning, on one side, and to identify incompatibilities and possibilities for integration with the current theories of cultural evolution, on the other.


## Conclusion

Our goal in this chapter has been to provide a clearer view of what would be required in order to understand the transformation of utterance meanings in short-term cultural evolution, be it in online quotations or in artificial transmission chains.
We first took a closer qualitative look at how meaning can evolve in chains by discussing a number of examples that appeared in the experiments of the previous chapter.
We then broadened the horizon and presented two prominent but radically different approaches to linguistic meaning that could provide insight into such changes:
Relevance Theory, which bases its account on the inference of relevant conclusions given the context in which an utterance is pronounced;
and the Enactive approach, which bases its account on the agent's sense-making activity by which it maintains its identity as an agent.
While the two approaches sit at opposite ends of the computational-dynamical spectrum in cognitive science, both base their account of meaning on some notion of the relevance that interactions have for agents (though those notions are very different for the two theories).
Notwithstanding the simpler integration of RT with Cultural Attraction Theory, we proposed that both RT and the Enactive approach could be valuable paths to explore in the study of meaning in this context.
Finally, we extended a few speculations as to how these approaches could inspire future work aimed at understanding the way meanings evolve along with cultural evolution.

Our stance may be taken as very ecumenical, too much so maybe.
How could it make sense to try and combine approaches that rely on bodies of philosophical works that continuously butcher each other?
In their current state, neither Relevance Theory nor the Enactive approach provide complete accounts of what linguistic meaning is or how we understand it, but both provide extremely valuable steps to begin understanding the phenomenon.
Our belief is that philosophical butchery is a healthy and necessary activity, and empirical combination and contrasting of the outcomes is too.
While the two approaches clearly have disagreements on starting points and approaches to cognition overall, it is not said that different approaches to meaning, when fully formalised at the right level, could not be related to one another.
In the meantime, both can inspire valuable empirical work and the design of new methods, be it to contrast them or to combine them, all of which will contribute to better understanding the way meaning changes when repeatedly transmitted or understood by different people.
